# MCStatus Project Tasks

## PHASE 0 ‚Äî Infraestructura y Base S√≥lida
- [ ] 0.1 Limpieza general del repo
  - [ ] Evaluar estructura actual de carpetas
  - [ ] Crear documento ARCHITECTURE.md
  - [ ] Normalizar imports y dependencias
- [ ] 0.2 Ambiente y dependencias
  - [ ] Evaluar migraci√≥n a poetry/uv
  - [ ] Configurar pre-commit hooks
  - [ ] A√±adir type hints con mypy
- [ ] 0.3 Base de datos
  - [ ] Evaluar migraci√≥n a PostgreSQL
  - [ ] Configurar Alembic para migraciones
  - [ ] Documentar esquema de tablas

## PHASE 1 ‚Äî Scraping Profesional
- [ ] 1.1 Proxy Pool + Rotaci√≥n
  - [ ] Implementar sistema de rotaci√≥n de proxies
  - [ ] Configurar backoff exponencial
- [ ] 1.2 Scraper Dual
  - [ ] Mejorar scraper requests existente
  - [ ] Completar scraper Selenium
  - [ ] Implementar sistema de fallback autom√°tico
- [ ] 1.3 Sistema de Colas
  - [ ] Crear worker pool con l√≠mite de concurrencia
  - [ ] Implementar sistema de prioridad
- [ ] 1.4 M√©tricas del Scraper
  - [ ] Crear tabla de m√©tricas
  - [ ] Dashboard de scraping stats

## PHASE 2 ‚Äî Detecci√≥n Premium/Semi-Premium/Cracked
- [/] 2.1 Fingerprinting por protocolo (EN PROGRESO)
  - [x] Implementar DeepProtocolAnalyzer b√°sico
  - [ ] A√±adir an√°lisis de timing
  - [ ] Capturar byte signatures
- [/] 2.2 Heur√≠sticas basadas en plugins (PARCIAL)
  - [x] Detectar plugins de auth b√°sicos
  - [ ] A√±adir BungeeCord, Geyser, etc.
- [ ] 2.3 Scoring probabil√≠stico
  - [ ] Implementar sistema de scoring
  - [ ] Calibrar umbrales
- [ ] 2.4 Registro de evidencia
  - [ ] Guardar raw handshakes
  - [ ] Log de heur√≠sticas disparadas
- [ ] 2.5 Revisi√≥n manual
  - [ ] Panel de inspecci√≥n de servidor
  - [ ] Botones de relabel/flag

## PHASE 3 ‚Äî Deduplicaci√≥n Enterprise
- [x] 3.0 Base de deduplicaci√≥n (COMPLETADO)
- [ ] 3.1 Nuevas estrategias
  - [ ] Player Sample Matching
  - [ ] MOTD Fuzzy Matching
  - [ ] Geolocalizaci√≥n
  - [ ] Fingerprint temporal
- [ ] 3.2 Motor de canonicalizaci√≥n
  - [ ] Implementar reglas de selecci√≥n
- [ ] 3.3 UI de Dedup Admin
  - [ ] Panel de gesti√≥n de duplicados
  - [ ] Historial de merges
- [ ] 3.4 Dedupe Pipeline
  - [ ] Pipeline diario automatizado
  - [ ] Sistema de reportes

## PHASE 4 ‚Äî API + Dashboard PRO
- [x] 4.0 API b√°sica (COMPLETADO)
- [ ] 4.1 Mejoras API
  - [ ] Nuevos endpoints (history, aliases, conflicts)
  - [ ] Implementar Redis cache
  - [ ] Filtros avanzados
- [ ] 4.2 Dashboard Admin
  - [ ] Servers Explorer
  - [ ] Deduplication Center
  - [ ] Detection Analyzer
  - [ ] Scrape Metrics
- [ ] 4.3 Dashboard P√∫blico
  - [ ] Rankings y tendencias
  - [ ] Filtros avanzados

## PHASE 5 ‚Äî Data Science / Inteligencia
- [ ] 5.1 Modelos predictivos
  - [ ] Detecci√≥n de crecimiento
  - [ ] Clustering por tipo
- [ ] 5.2 Patrones sospechosos
  - [ ] Detecci√≥n de bots
  - [ ] Redes ocultas
- [ ] 5.3 Historial temporal
  - [ ] An√°lisis de evoluci√≥n
- [ ] 5.4 Export y Data Layer
  - [ ] CSV/JSON exports
  - [ ] Dataset p√∫blico

## PHASE 6 ‚Äî Opcionales Premium
- [ ] 6.1 Discord bot
- [ ] 6.2 Plugin Minecraft
- [ ] 6.3 Rate-limiting API
- [ ] 6.4 Caching distribuido

## üî• OPCI√ìN A - QUICK WINS (Semana 1) - PRIORIDAD ALTA

### Task 1: Sistema de Fallback Autom√°tico para Scraping
- [ ] 1.1 Crear `core/adaptive_scraper.py`
  - [ ] Clase AdaptiveScraper con estrategias m√∫ltiples
  - [ ] M√©todo fast_scrape() con requests
  - [ ] M√©todo selenium_scrape() con headless browser
  - [ ] M√©todo rotate_proxy() integrando proxy_manager
  - [ ] Sistema de backoff exponencial
- [ ] 1.2 Crear excepciones personalizadas (RateLimitError, ScraperError)
- [ ] 1.3 Refactorizar scrapers existentes
  - [ ] scrape_namemc_600.py
  - [ ] scrape_namemc_enterprise.py
- [ ] 1.4 Crear tests/test_adaptive_scraper.py

### Task 2: Scoring Probabil√≠stico Semi-Premium
- [ ] 2.1 Modificar core/enterprise/detector.py
  - [ ] A√±adir clase DetectionEvidence
  - [ ] A√±adir m√©todo _calculate_score()
  - [ ] Modificar detect() para retornar evidencias
- [ ] 2.2 Crear config/detection_weights.yaml
- [ ] 2.3 Implementar almacenamiento de evidencias en metadata
- [ ] 2.4 Crear tests/test_detection_scoring.py

### Task 3: Dashboard de M√©tricas de Scraping
- [ ] 3.1 Crear migraci√≥n 002_scrape_metrics.sql
- [ ] 3.2 Crear core/scrape_metrics.py
  - [ ] M√©todo log_scrape_attempt()
  - [ ] M√©todo get_metrics()
- [ ] 3.3 A√±adir endpoint /api/scraping/metrics en core/api.py
- [ ] 3.4 Crear web/static/scraping_metrics.html
- [ ] 3.5 Crear web/static/scraping_metrics.js con Chart.js
- [ ] 3.6 Integrar logging en AdaptiveScraper

## üéØ Decisi√≥n Inmediata Requerida
**¬øQu√© task de Opci√≥n A empezamos primero?**
- [ ] Task 1: Scraping con fallback (desbloquea scraping inmediatamente)
- [ ] Task 2: Scoring probabil√≠stico (mejora detecci√≥n YA)
- [ ] Task 3: Dashboard m√©tricas (visibilidad del sistema)
